---
title: "TMA4250-Project-2"
author: "Ole Riddervold, Ole Kristian Skogly"
date: "2023-03-06"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# install.packages("ggnewscale")
# install.packages("scales")

library(spatial)
library(MASS)
library(ggplot2)
library(patchwork)
library(glue)
library(tidyverse)
library(reshape2)
library(ggnewscale)
library(scales)
library(latex2exp)
library(Orcs)
```


## Loading data
```{r}
cells   <- read.table("data/cells.dat", skip=3, col.names=c('x', 'y'))
redwood <- read.table("data/redwood.dat", skip=3, col.names=c('x', 'y'))
pines   <- read.table("data/pines.dat", skip=3, col.names=c('x', 'y'))
```

## Exploring data
```{r}
head(cells)
head(redwood)
head(pines)
```


# Problem 1
## a)
```{r}
display_point_patterns <- function(dataframes, titles, pts=1, txs=11) {
  # Plots point patterns for dataframes with point data
  # Args: Dataframes and titles as lists of the same lengths
  # Returns: None
  n <- length(dataframes)
  do.call(
    wrap_plots,
    lapply(1:n, function(i) {
      ggplot(dataframes[[i]]) + geom_point(aes(x, y), size=pts) +
        xlab('x [m]') + ylab('y [m]') + labs(title=titles[i]) +
        coord_fixed() + theme(text=element_text(size=txs))
    })
  )
}
```

```{r}
plot_titles = c(
  "Cells",
  "Redwood",
  "Pines"
)

png("figures/1_a.png", width=3600, height=1200, units="px", res=300) # Saving

display_point_patterns(list(
  cells,
  redwood,
  pines
), plot_titles)
```
For the Cells dataset, the points seem to be uniformly spread across the whole window. This would make sense since cells exist in very large quantities, and repel each other to a certain degree (https://www.hcplive.com/view/cells_repel). This will result in a uniform distribution of points when we pick a small window.

Comparing the last two dataset, it is would be reasonable to hypothesize that trees have a tendency to grow in clusters along streams and other bodies of water, which is very evident in the point pattern plotted above. Pines, however, seem to follow a more random pattern of growth, giving the random pattern that is seen in the rightmost plot of Figure (FIGREF).

These hypotheses are prime candidates for fitting and inference using spatial point processes, which is what will be studied throughout this project. The project will also concern inference in cases with limited observation probabilities of points.

## b)
The L-function is defined by
\begin{equation}
  L(r) = \sqrt[d]{\frac{K(r)}{b_d}}
\end{equation}

where
\begin{align}
  K(r) &= \frac{1}{\lambda}\mathbb{E}_\mathbf{0}\left[N\left(B_r(\mathbf{0}\backslash\{\mathbf{0}\})\right)\right] \\
  b_d = \nu(B_1(\mathbf{0}))
\end{align},
$B_r(\mathbf{x})$ denotes a ball of radius $r$ centered in $\mathbf{x}$ in the relevant space (here $\mathbb{R}^2$), and $\nu$ denotes the volume-function. The K-function $K(r)$ can be interpreted as the ratio between the expected number of points in a ball centered at $\mathbf{0}$ excluding the origin itself, *given* that there is a point at the origin, and the rate of the point process itself. Therefore one can say that the L-function is a variant of the K-function that takes the curse of dimensionality, i.e. the fact that points get further apart the larger the dimension, into account. In $\mathbb{R}^2$, we have
\begin{equation}
  L(r) = \sqrt(\frac{K(r)}{\pi})
\end{equation}

Estimation of the L-function:
```{r}
pp.cells   <- ppinit("data/cells.dat")
pp.redwood <- ppinit("data/redwood.dat")
pp.pines   <- ppinit("data/pines.dat")

L.cells   <- data.frame(Kfn(pp.cells, fs=1.0)[1:2])
L.redwood <- data.frame(Kfn(pp.redwood, fs=1.0)[1:2])
L.pines   <- data.frame(Kfn(pp.pines, fs=1.0)[1:2])
```

L-function plotting:
```{r}
display_L_functions <- function(dataframes, titles) {
  # Plots K-functions and affine lines with growth 1 for comparison
  # Args: Dataframes and titles as same length lists
  # Returns: None
  n <- length(dataframes)
  do.call(
    wrap_plots,
    lapply(1:n, function(i) {
      ni <- nrow(dataframes[[i]])
      df <- dataframes[[i]] %>% mutate(xline=1:ni/100, yline=1:ni/100)
      ggplot(df) + geom_point(aes(x, y), size=1, col="red") +
        xlab('r') + ylab('L(r)') + labs(title=titles[i]) +
        geom_line(aes(xline, yline)) +
        coord_fixed()
    })
  )
}
```


```{r}
png("figures/1_b.png", width=3600, height=1200, units="px", res=300) # Saving

display_L_functions(list(
  L.cells,
  L.redwood,
  L.pines
), titles=plot_titles)
```

If, for small distances, the L-function diverges from the theoretical straight function (a straight line), then that is a implication that there is either clustering or repulsion in the dataset. For each of the datasets provided:
- Cells: The L-function goes below the theoretical L-function, meaning that there are fewer points than expected in the ball with small $r$. This implies that there is repulsion in the point process, and a Gibbs process might be more suitable.
- Redwood: The L-function is slightly higher than the theoretical function for small $r$, which is an implication of clustering, as discussed in 1a), and a Neymann-Scott process may therefore be more suitable.
- Pines: The estimated L-function is mostly a straight line corresponding to the theoretical function, implying that a Poisson point process is a suitable model.


## c)
For a homogeneous Poisson point process on a window $W\subseteq \mathbb{R}^m$, we have the convenient result:
\begin{equation}
  \left[\mathbf{x}_1. \cdots, \mathbf{x}_n | N(W)=n\right] \overset{\text{i.i.d.}}{\sim} U(W)
\end{equation}

In this case, $m=2$ and $W=[0, 1]\times[0, 1]$. The points are therefore very easy to simulate, as they follow a uniform distribution on $W$.
```{r}
simulate_R2_unif <- function(n, m) {
  # Simulates a homogeneous Poisson point process on R^2 in m realizations on [0, 1]x[0, 1].
  # Args: n (amount of points to simulate in each realization) and m (amount of realizations)
  # Returns: mxnx2 array.
  dims <- c(m, n, 2)
  realizations <- array(runif(m*n*2), dims)
  return(realizations)
}
```

```{r}
calculate_R2_L_functions <- function(realizations) {
  # Given realizations of some point process in the form of an array, calculates
  # the L-function for each realiztion and returns as an array.
  # Args: realizations (mxnx2 array)
  #         - m: Amount of realzations
  #         - n: Amount of points per realization
  # Returns: mxn-array
  d <- dim(realizations)
  L <- array(rep(0, prod(d[1:2])), d[1:2])
  for (i in 1:d[1]) {
    df <- data.frame(realizations[i, , ])
    colnames(df) <- c('x', 'y')
    L[i, ] <- Kfn(df, fs=1.0)$y
  }
  return(L)
}
```

```{r}
R2_L_function_stats <- function(L_functions) {
  # Given an array of m L-functions at n points, calculates mean and 90%
  # confidence intervals.
  # Args: L_functions (mxn)
  # Returns: Dataframe of L-functions
  stats <- L_functions %>% melt() %>%
    group_by(Var2) %>%
    summarize(
      mean=mean(value),
      CI90=qt(0.05, length(value))*sd(value)*sqrt(1 + 1/length(value))
    )
  return(stats)
}
```

```{r}
simulate_R2_ppp_Lfn <- function(n, m) {
  # Simulates a Poisson point process conditional on N = n, and returning
  # a dataframe containing empirically estimated L-function and 90% CI.
  return(
    simulate_R2_unif(n, m) %>%
      calculate_R2_L_functions() %>%
      R2_L_function_stats()
  )
}
```

```{r}
display_L_functions_with_ppp_sim <- function(dataframes, titles, m=100) {
  # Plots K-functions and affine lines with growth 1 for comparison
  # Args: Dataframes and titles as same length lists
  # Returns: None
  n <- length(dataframes)
  do.call(
    wrap_plots,
    lapply(1:n, function(i) {
      ni <- nrow(dataframes[[i]])
      simi <- simulate_R2_ppp_Lfn(n=ni, m=m)
      df <- dataframes[[i]] %>% mutate(Lmean=simi$mean, LCI=simi$CI90)
      ggplot(df) + geom_point(aes(x, y), size=1, col="red") +
        geom_line(aes(x, Lmean)) +
        geom_ribbon(aes(x, ymin=Lmean-LCI, ymax=Lmean+LCI), alpha=0.25) +
        xlab('r') + ylab('L(r)') + labs(title=titles[i]) +
        coord_fixed() + theme(text=element_text(size=11))
    })
  )
}
```

```{r}
png("figures/1_c.png", width=3600, height=1200, units="px", res=300) # Saving

display_L_functions_with_ppp_sim(list(
  L.cells,
  L.redwood,
  L.pines
), titles=plot_titles)
```


# Problem 2
## a)
```{r}
obspines <- read.table("data/obspines.txt", skip=1, col.names=c('x', 'y', 'obs'))
obsprob <- read.table("data/obsprob.txt", skip=1, col.names=c('x', 'y', 'prob'))
```


```{r}
png("figures/2_a1.png", width=2400, height=1900, units="px", res=300) # Saving
ggplot(obspines) +
  geom_tile(aes(x, y, fill=obs)) + scale_fill_gradient(low="white", high="purple") +
  xlab('x [m]') + ylab('y [m]') + 
  coord_fixed() + theme(text=element_text(size=10))

png("figures/2_a2.png", width=2400, height=1900, units="px", res=300) # Saving
ggplot(obsprob) + 
  geom_tile(aes(x, y, fill=prob)) + scale_fill_gradient(low='white', high='darkgreen') + 
  xlab('x [m]') + ylab('y [m]') +
  coord_fixed() + theme(text=element_text(size=10))
```


## b)
Assuming that the pine trees follow a Poisson point process, which supports the conclusions made in 1). It can be proved that if $\alpha: W \rightarrow[0,1]$ denotes detection probabilities on $W$ (which in this case is defined as $W = [0, 300]\times[0, 300]$), and $N(\mathbf{x})$ is a homogeneous Poisson P.P. with rate $\lambda_N$ then the observed points $M(\mathbf{x})$ follow an inhomogeneous Poisson P.P. with rate given by
\begin{equation}
  \lambda_M(\mathbf{x}) = \lambda_N \alpha(\mathbf{x}) \: \:\forall \: \mathbf{x} \in W
\end{equation}
Thus, we get that
\begin{align}
  M | N &\sim \mathrm{Poisson}(\nu_{\lambda_M}(W)) \\
  \nu_{\lambda_M}(W) &= \lambda_N\int_W{\alpha_(\mathbf{x})\mathrm{d}\mathbf{x}}
\end{align}
Consequently, the point locations are distributed uniformly with pdf
\begin{equation}
  f(\mathbf{x}_1, \cdots, \mathbf{x}_m | M(W)=m) = \prod_{i=1}^m{\frac{\lambda_M(\mathbf{x}_i)}{\nu_{\lambda_M}(W)}} = \left(\frac{\lambda_N}{\nu_{\lambda_M}(W)}\right)^m\prod_{i=1}^m{\alpha(\mathbf{x}_i)}
\end{equation}

## c)
We want to determine $C$ such that
\begin{equation}
  \hat{\Lambda} = \frac{1}{C}\sum_{i, j}{M_{ij}}
\end{equation}
is unbiased.

Estimate from data:
```{r}
remote_estimate_lambda <- function(obs, prob, grid_size) {
  # Estimates lambda based on observed points and detection probabilities
  # Args: obs (Amount of observations in a gridpoint), alpha (detection probabilities)
  # Returns: (float) Estimate of lambda
  C <- grid_size*sum(prob) # Normalizing constant
  lambda_hat <- sum(obs)/C
  return(lambda_hat)
}
```

```{r}
grid_size <- 10^2 # For this particular problem
lambda_hat <- remote_estimate_lambda(obspines$obs, obsprob$prob, grid_size)
lambda_hat
```

With the given data, the estimate is approximately $\hat{\lambda}_N = 0.0104$.

The following algorithm takes a grid in the form of a dataframe with x- and y-coordinates and corresponding rate $\lambda$, and simulates a homogeneous P.P.P. in each point using the rate.

```{r}
simulate_pois_dyn_rate <- function(lambda, displacement = 0) {
  # Given some changing rate lambda, draws corresponding amount of samples
  # for each rate.
  # Args:
  #   lambda (array-like): Varying rate of size m
  # Returns:
  #   Poisson samples of size m
  k <- length(lambda)
  return(
    lapply(
      1:k,
      function(i) rpois(1, lambda[i])
    ) %>% unlist() + displacement
  )
}

simulate_ppp_on_grid <- function(grid, poisson_displacement=0) {
  # Simulates a inhomogeneous Poisson point process on a square grid.
  # Args:
  #   grid  (Dataframe): Coordinates and rate (columns x, y, lambda)
  #   cell_size (float): Length of the given cell
  # Returns:
  #   Matrix of points
  k <- nrow(grid)
  n <-  simulate_pois_dyn_rate(grid$lambda, poisson_displacement) # BETTER WAY OF DOING THIS?
  points <- array(
    runif(2*sum(n), min=0, max=cell_size),
    c(sum(n), 2)
  ) %>% data.frame()
  colnames(points) <- c('x', 'y')

  # Adding x- and y-values to each point:
  ind <- 0
  for (i in 1:k) {
    if (n[i] > 0) {
      for (j in 1:n[i]) {
        points$x[ind+j] <- points$x[ind+j] + grid$x[i]
        points$y[ind+j] <- points$y[ind+j] + grid$y[i]
      }
      ind <- ind + n[i]
    }
  }
  return(points)
}
```

```{r}
cell_size <- 10
prior.grid <- obsprob %>% select(c('x', 'y')) %>% mutate(lambda=cell_size^2*lambda_hat)
prior.points_homogeneous_ppp <- replicate(3, simulate_ppp_on_grid(prior.grid), simplify=FALSE)
```

```{r}
png("figures/2_c.png", width=3600, height=1200, units="px", res=300) # Saving

display_point_patterns(
  prior.points_homogeneous_ppp,
  titles=c("", "", ""),
  txs=11
)
```

## d)
```{r}
posterior.grid <- obsprob %>% mutate(lambda=cell_size^2*lambda_hat*(1-prob)) %>% select(-prob)
posterior.points_inhomogeneous_ppp <- replicate(3, simulate_ppp_on_grid(
  posterior.grid, 
  poisson_displacement=obspines$obs
), simplify=FALSE)
```

```{r}
png("figures/2_d.png", width=3600, height=1200, units="px", res=300) # Saving

display_point_patterns(
  posterior.points_inhomogeneous_ppp,
  titles=c("", "", ""),
  txs=11
)
```

It is clear that this point pattern is slightly more centered towards the "unknown" lower left of $W$, the area where $\alpha$ is small, and points are harder to detect. Nevertheless, even though the process is centered around this area due to the rate given by $\lambda_N(1-\alpha(\mathbf{x}))$, the points are still mostly uniformly spread, since the Poisson distribution for $\mathbf{N} | \mathbf{M}$ is displaced by $m$, which needs to be compensated for after simulation from the distribution by adding the observations $\mathbf{M}$. Naturally, $\mathbf{M}$ will tend to be larger where $\alpha$ is larger, which evens out the skew which can be found in the derived distribution.

## e)
```{r}
n <- 500

# Prior:
prior.sim <- replicate(n, simulate_pois_dyn_rate(prior.grid$lambda)) %>% melt() %>% # Simulating and creating a dataframe
  group_by(Var1) %>% summarize(mean=mean(value), std=sd(value)) %>%                 # Grouping and getting mean and std
  bind_cols(obsprob %>% select(c('x', 'y'))) %>% select(-Var1)                      # Concatenating grid coordinates and removing index
prior.sim

# Posterior:
posterior.sim <- replicate(n, simulate_pois_dyn_rate(posterior.grid$lambda, displacement=obspines$obs)) %>% melt() %>%
  group_by(Var1) %>% summarize(mean=mean(value), std=sd(value)) %>%
  bind_cols(obsprob %>% select(c('x', 'y'))) %>% select(-Var1)
posterior.sim
```

```{r}
df_merged <- merge(prior.sim, posterior.sim, by=c('x', 'y'))
colnames(df_merged)[3:length(df_merged)] <- c("prior_mean", "prior_std", "posterior_mean", "posterior_std")
# df_merged

generate_boundary_indices <- function(fnc) {
  # Generates either min or max for mean and std respectively to fit the
  # plotting scheme below of df_merged.
  val <- rep(0, 4)
  val[c(1, 3)] <- df_merged %>% select(c('prior_mean', 'posterior_mean')) %>% fnc()
  val[c(2, 4)] <- df_merged %>% select(c('prior_std', 'posterior_std')) %>% fnc()
  return(val)
}

# Mins and maxes for each of the four plots:
mins <- generate_boundary_indices(min)
maxs <- generate_boundary_indices(max)
col.lwr <- c("white", "green", "white", "green") # Thoughts: Low std: Good (green)
col.upr <- c("purple", "red", "purple", "red")   # Thoughts: High std: Bad (red)

png("figures/2_e.png", width=2400, height=1800, units="px", res=300) # Saving

do.call(
  wrap_plots,
  lapply(1:4, function(i) {
    df_merged %>% {
      ggplot(.) +
        geom_tile(aes_string(x='x', y='y', fill=names(.)[i+2])) +
        scale_fill_gradient(low=col.lwr[i], high=col.upr[i],limits=c(mins[i], maxs[i])) +
        xlab('x [m]') + ylab('y [m]') + labs(cbar="test") +
        coord_fixed() + theme(text=element_text(size=9))
    }
  })
)
```
There is a clear distinction between the prior Poisson point process and the posterior, in that the gradient caused by the detection probabilities is very apparent. One can also see that there is significantly less variance in general for the posterior values, especially at the top right, where there is a high detection probability.

```{r, echo=FALSE, eval=FALSE}
df_merged <- merge(prior.sim, posterior.sim, by="Var2")
xlabels <- c(TeX("Prior $\\hat{\\mu}$"), TeX("Prior $\\hat{\\sigma}$"), TeX("Posterior $\\hat{\\mu}$"), TeX("Posterior $\\hat{\\sigma}$"))

xmin <- df_merged %>% select(-Var2) %>% min()
xmax <- df_merged %>% select(-Var2) %>% max()

# png("figures/2_e.png", width=1600, height=1200, units="px", res=300) # Saving

do.call(
  wrap_plots,
  lapply(1:4, function(i) {
    df_merged %>% {
      ggplot(.) +
      geom_histogram(aes_string(x=names(.)[i+1]), bins=40) +
      geom_vline(xintercept=mean(.[,i+1]), col="red") +
      xlim(xmin, xmax) + xlab(xlabels[i])
    }
  })
)
```


# Problem 3
```{r, echo=FALSE, eval=FALSE}
# Read in data
redwood <- read.table("data/redwood.dat", skip=3, col.names = c("x", "y"))
# Plot the data
#plot(redwood)

# Function to simulate Neyman-Scott process 
SimulateNSP <- function(lambda_p, lambda_d, sigma, xl = 0, xu = 1, yl = -1, yu = 0, min_d = 0) {
  # Calculate the size of the extended window
  ext_window_x <- c(xl - 2 * sigma, xu + 2 * sigma)
  ext_window_y <- c(yl - 2 * sigma, yu + 2 * sigma)
  ext_window_area <- diff(ext_window_x) * diff(ext_window_y)
  
  # Generate the parent points using a Poisson process
  n_p <- rpois(1, lambda_p * ext_window_area)
  x_p <- runif(n_p, ext_window_x[1], ext_window_x[2])
  y_p <- runif(n_p, ext_window_y[1], ext_window_y[2])
  
  # Initialize arrays to store daughter points
  x_d <- numeric()
  y_d <- numeric()
  
  # Generate daughter points around each parent point
  #browser()
  for (i in seq_along(x_p)) {
    n_d <- rpois(1, lambda_d)
    if (n_d > 0) {
      # Generate n_d daughter points around parent point i
      mu <- c(x_p[i], y_p[i])
      cov_mat <- sigma^2 * diag(2)
      # ! error on line under when n_d = 1
      d_points <- matrix(mvrnorm(n_d, mu, cov_mat), nrow = length(n_d))
      if (nrow(d_points) > 0) {  # check if d_points has more than one row
        x_d <- c(x_d, d_points[, 1])
        y_d <- c(y_d, d_points[, 2])
      }
    } else { # add min_d points if n_d is zero
      x_d <- c(x_d, rep(x_p[i], min_d))
      y_d <- c(y_d, runif(min_d, yl, yu))
    }
  }
  #browser()
  # Remove daughter points outside the specified window
  # changed the y coordinated, cannot check for -1 and 0 for y-axes here
  keep_idx <- which(x_d >= xl & x_d <= xu & y_d >= 0 & y_d <= 1)
  x_d <- x_d[keep_idx]
  y_d <- y_d[keep_idx]
  
  # Return a list of daughter points
  return(list(x = x_d, y = y_d))
}

options(error = recover)
# Set random seed for reproducibility
set.seed(4250)

# Guestimate parent intensity
lambda_p = 9

# Guestimate daughter intensity
lambda_d = 7

# Guestimate Gaussian kernel standard deviation
sigma = 0.05

# Number of realizations
nsim <- 100

# Plot a realization from the guestimated Neyman-Scott process
plot(SimulateNSP(lambda_p,lambda_d,sigma), xlab = 'x', ylab= 'y',  main = "A realization from the guestimated Neyman-Scott process")

# Convert data to spatstat point pattern object
#ppRedwood <- ppinit("redwood.dat")

# Define study area
ppregion(0, 1, 0, 1)
#ppregion(0,1,-1,0)

# Compute empirical L-function for observed
LRedwood <- Kfn(redwood, fs = 1)

# Construct a matrix to store the L-function values for each realization
Lsimulations = matrix(NA, nrow = nsim, ncol = 70)

for (i in 1:nsim) {
  sim_L <- SimulateNSP(lambda_p, lambda_d, sigma, xl = 0, xu = 1, yl = -1, yu = 0, min_d = 0)
  L_NS_temp <- Kfn(sim_L, 1, 100)
  Lsimulations[i,] <- L_NS_temp$y
}


# Utility for summary statistics dataframe:
sim_L_summary <- function(sim) {
#   Given a matrix n by m of n realizations of simulation
#   producing m values of L-fnc, calculates mean, and 90% Pi.
  L <- sim %>%
  melt(varnames=c('rel', 'r')) %>%
  mutate(r=r/max(r)) %>%
  group_by(r) %>%
  summarise(
    L_mean=mean(value),
    PI90_lwr=quantile(value, 0.05),
    PI90_upr=quantile(value, 0.95)
  )
  return(L)
}


# Summarizing statistics:
redwood.L <- sim_L_summary(Lsimulations)

#Plot redwood data and three realizations
par(mfrow=c(2,2)) # Set up 2x2 grid of plots
plot(redwood, main="Redwood data") # First plot
for (i in 1:3) {
  sim_NSP_3realizations = SimulateNSP(lambda_p,lambda_d,sigma)
  plot(sim_NSP_3realizations, main=paste("Simulated realization", i), xlab='x', ylab='y', xlim=c(0,1), ylim=c(0,1)) # Subsequent plots
}

# Plot
par(mfrow=c(1,1))
ggplot(redwood.L) + geom_line(aes(x=r, y=L_mean), col="red") +
  geom_ribbon(aes(x=r, ymin=PI90_lwr, ymax=PI90_upr), alpha=0.2) +
  ylab("L")

# Old code:
# plot(LRedwood$x, LRedwood$y, col = "blue", type = "l", lwd = 2, xlab = "r", ylab = "L(r)", main = "Plot of empirical L-function with empirical prediction interval from guestimated model.")
# lines(LRedwood$x, L_sims_upper, type = "l", col = "red", lwd = 2, lty = 2)
# lines(LRedwood$x, L_sims_lower, type = "l", col = "red", lwd = 2, lty = 2)
# for(i in 1:nsim){
#   lines(LRedwood$x,Lsimulations[i,], type = "l", col = "yellow", lwd = 2, lty = 2) 
# }
# legend('topleft', cex = 0.7, c("90% prediction interval from guestimated model", "Empirical L-function from the data"), lty=c(2,1), col = c("red", "blue"), lwd = c(1.5,2))
```


# Problem 4
```{r, echo=FALSE, eval=FALSE}
# Read in data
cells <- read.table("data/cells.dat", skip=3, col.names=c('x', 'y'))
# Plot the data
#plot(cells)

ppCells <- ppinit("cells.dat")
n_Strauss <- length(ppCells$x)

ppregion(0,1,0,1)
LCells <- Kfn(ppCells, fs=1)


#guessed parameters
r_Strauss <- 0.01
beta_Strauss <- 5
c_Strauss <- exp(-beta_Strauss)

# Number of realizations
nsim <- 100

# Construct a matrix to store the L-function values for each realization
Lsimulations_Strauss = matrix(NA, nrow = nsim, ncol = 70)

L_StraussUpper = rep(NA,70)
L_StraussLower = rep(NA,70)

ppregion()

#s <- Strauss(n_Strauss, c_Strauss, r_Strauss)
#plot(s)

for (i in 1:nsim) {
  sim_Strauss <- Strauss(n_Strauss,c_Strauss, r_Strauss)
  L_Strauss_temp <- Kfn(sim_Strauss, 1, 100)
  Lsimulations_Strauss[i,] <- L_Strauss_temp$y
}

# Summarizing statistics:
cells.L <- sim_L_summary(Lsimulations_Strauss)

#Plot of the cells data and the three simulated realizations
par(mfrow=c(2,2)) # Set up 2x2 grid of plots
plot(cells, main="Cells data") # First plot
for (i in 1:3) {
  sim_Strauss_3realizations = Strauss(n_Strauss, c_Strauss, r_Strauss)
  plot(sim_Strauss_3realizations, main=paste("Simulated realization", i), xlab='x', ylab='y', xlim=c(0,1), ylim=c(0,1)) # Subsequent plots
}

# Plot
par(mfrow=c(1,1))
ggplot(cells.L) + geom_line(aes(x=r, y=L_mean), col="red") +
  geom_ribbon(aes(x=r, ymin=PI90_lwr, ymax=PI90_upr), alpha=0.2) +
  ylab("L")


# Old code:
# Find upper and lower prediction intervals
# for (i in (1:70)){
#   L_StraussUpper[i] = quantile(Lsimulations_Strauss[,i], 0.95)
#   L_StraussLower[i] = quantile(Lsimulations_Strauss[,i], 0.05)
# }

# plot(LCells$x, LCells$y, col = "blue", lwd = 1, ylim = c(0,0.8))
# lines(LCells$x, L_StraussUpper, type = "l", col = "red", lwd = 2, lty = 2)
# lines(LCells$x, L_StraussLower, type = "l", col = "red", lwd = 2, lty = 2)
# legend('topleft', cex = 0.7, c("90% prediction interval from guestimated model", "Empirical L-function from the data"), lty=c(2,1), col = c("red", "blue"), lwd = c(1.5,2))

```

